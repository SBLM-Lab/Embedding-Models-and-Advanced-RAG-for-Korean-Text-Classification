{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sGHqBo4N4it_"
   },
   "source": [
    "## Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import logging\n",
    "import warnings\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from pydantic import BaseModel, Field\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.schema import Document\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain.callbacks import get_openai_callback\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=1\n",
    "retrieve_k = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2rEYrdZc4iuA"
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MAXVJsdL4iuA"
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"민원_train.csv\", encoding=\"utf-8\")\n",
    "test_df = pd.read_csv(\"민원_validation.csv\", encoding=\"utf-8\")\n",
    "\n",
    "combined_df = pd.concat([train_df, test_df], ignore_index=True)\n",
    "combined_df = combined_df.drop_duplicates(subset=\"Q_refined\", keep=\"first\")\n",
    "\n",
    "min_threshold = 1600\n",
    "fixed_sample_size = 1600\n",
    "\n",
    "balanced_df = pd.DataFrame()\n",
    "category_counts = combined_df['predication'].value_counts()\n",
    "\n",
    "for category, count in category_counts.items():\n",
    "    subset = combined_df[combined_df['predication'] == category]\n",
    "    if count < min_threshold:\n",
    "        continue  \n",
    "    else:\n",
    "        subset = subset.sample(fixed_sample_size, random_state=42)\n",
    "    balanced_df = pd.concat([balanced_df, subset])\n",
    "\n",
    "test_size = 1000\n",
    "train_data, test_data = train_test_split(balanced_df, test_size=test_size, stratify=balanced_df['predication'], random_state=seed)\n",
    "\n",
    "val_size = 200\n",
    "train_data, validation_data = train_test_split(train_data, test_size=val_size, stratify=train_data['predication'], random_state=seed)\n",
    "\n",
    "train = train_data.copy()\n",
    "validation = validation_data.copy()\n",
    "test = test_data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j2bqeHqu4iuA"
   },
   "source": [
    "## Vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "\n",
    "class KUREEmbedding:\n",
    "    def __init__(self, model_name=\"nlpai-lab/KURE-v1\"):\n",
    "        self.model = SentenceTransformer(model_name, trust_remote_code=True).to(device)\n",
    "\n",
    "    def embed_documents(self, texts):\n",
    "        embeddings = self.model.encode(texts, convert_to_numpy=True)\n",
    "        return embeddings\n",
    "\n",
    "    def embed_query(self, text):\n",
    "        return self.embed_documents([text])[0]\n",
    "\n",
    "class KoE5Embedding(KUREEmbedding):\n",
    "    def __init__(self, model_name=\"nlpai-lab/KoE5\"):\n",
    "        super().__init__(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore_path = f\"../seed{seed}/faiss_index_koe5_seed{seed}\"\n",
    "if os.path.exists(vectorstore_path):\n",
    "    embeddings = KoE5Embedding()\n",
    "\n",
    "    vectorstore = FAISS.load_local(\n",
    "        vectorstore_path,\n",
    "        embeddings.embed_query,\n",
    "        allow_dangerous_deserialization=True,\n",
    "    )\n",
    "\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": retrieve_k})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dAzYTg0X4iuC"
   },
   "source": [
    "## Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StatementClassification(BaseModel):\n",
    "    prediction: str = Field(description=\"Predicted classification label for the statement. Possible labels: 요청/개선, 문의(질의), 건의/제기, 항의, 고충/토로, 협조, 감사.\")\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "structured_llm_labeler = llm.with_structured_output(StatementClassification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt & Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiquery_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are an AI assistant that generates multiple rephrased versions of a given user query for better search retrieval.\"\n",
    "                \"Generate three alternative versions of the given query, ensuring that all queries are written in Korean only.\"\n",
    "                \"Limit the response to a maximum of 50 characters.\"),\n",
    "        (\"human\", \"Original query: {query}\")\n",
    "    ])\n",
    " \n",
    "retriever_from_llm = MultiQueryRetriever.from_llm(\n",
    "    retriever=vectorstore.as_retriever(),\n",
    "    llm=llm,\n",
    "    prompt=multiquery_prompt\n",
    ")\n",
    "\n",
    "classification_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"Classify the query into one of: 요청/개선, 문의(질의), 건의/제기, 항의, 고충/토로, 협조, 감사.\\n\"\n",
    "            \"Return in JSON: {{\\\"prediction\\\": \\\"category\\\"}}\"),\n",
    "    (\"human\", \"Query: {query}\\nRelevant cases: {similar_cases}\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nGbSqHfkVzu3"
   },
   "source": [
    "## Multi-Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zuCZsWlvU6z7"
   },
   "outputs": [],
   "source": [
    "def predict_label_with_multiquery_rag(statement: str, idx: int, total: int, max_docs: int = 5):\n",
    "\n",
    "    queries = [statement]\n",
    "    \n",
    "    formatted_prompt = multiquery_prompt.format(query=statement)\n",
    "    generated_queries = llm.invoke(formatted_prompt).content.strip().split(\"\\n\")\n",
    "    queries.extend(generated_queries)\n",
    "    \n",
    "    all_retrieved_docs = []\n",
    "    \n",
    "    for query in queries:\n",
    "        retrieved_docs = retriever_from_llm.invoke(query)[:max_docs]\n",
    "        all_retrieved_docs.extend(retrieved_docs)\n",
    "        \n",
    "    unique_docs = list({doc.page_content: doc for doc in all_retrieved_docs}.values())\n",
    "\n",
    "    retrieved_texts_str = \"\\n\".join([doc.page_content for doc in unique_docs])\n",
    "    \n",
    "    formatted_prompt = classification_prompt.format(query=statement, similar_cases=retrieved_texts_str)\n",
    "    \n",
    "    prediction = structured_llm_labeler.invoke(formatted_prompt)\n",
    "    \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j9VBQzHTlJr_"
   },
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for idx, row in tqdm(test.iterrows(), total=len(test), desc=\"Processing\"):\n",
    "    question = row[\"Q_refined\"]\n",
    "    actual_label = row[\"predication\"]\n",
    "    \n",
    "    prediction = predict_label_with_multiquery_rag(question, idx+1, len(test))\n",
    "\n",
    "    results.append(\n",
    "        {\"question\": question,\n",
    "         \"actual_label\": actual_label,\n",
    "         \"prediction_label\": prediction.predicted_label,\n",
    "        }\n",
    "    )\n",
    "\n",
    "df_results = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post-processing\n",
    "df_results['prediction_label'] = df_results['prediction_label'].replace({'고충': '고충/토로', '토로': '고충/토로'})\n",
    "df_results['prediction_label'] = df_results['prediction_label'].replace({'요청': '요청/개선', '개선': '요청/개선'})\n",
    "df_results['prediction_label'] = df_results['prediction_label'].replace({'건의': '건의/제기', '제기': '건의/제기'})\n",
    "df_results['prediction_label'] = df_results['prediction_label'].replace({'문의': '문의(질의)', '질의': '문의(질의)'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = df_results[\"actual_label\"]\n",
    "y_pred = df_results[\"prediction_label\"]\n",
    "\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision_macro = precision_score(y_true, y_pred, average=\"macro\")\n",
    "recall_macro = recall_score(y_true, y_pred, average=\"macro\")\n",
    "f1_macro = f1_score(y_true, y_pred, average=\"macro\")\n",
    "f1_weighted = f1_score(y_true, y_pred, average=\"weighted\")\n",
    "\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "classification_rep = classification_report(y_true, y_pred, output_dict=True)\n",
    "\n",
    "print(\"\\n===== Classification Performance Results =====\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision (Macro): {precision_macro:.4f}\")\n",
    "print(f\"Recall (Macro): {recall_macro:.4f}\")\n",
    "print(f\"F1-score (Macro): {f1_macro:.4f}\")\n",
    "print(f\"F1-score (Weighted): {f1_weighted:.4f}\")\n",
    "\n",
    "print(\"\\n===== Classification Confusion Matrix =====\")\n",
    "print(conf_matrix)\n",
    "\n",
    "print(\"\\n===== Detailed Classification Report =====\")\n",
    "print(classification_report(y_true, y_pred))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
