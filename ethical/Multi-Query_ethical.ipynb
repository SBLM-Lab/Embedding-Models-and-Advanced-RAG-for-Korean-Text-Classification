{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sGHqBo4N4it_"
   },
   "source": [
    "## Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import logging\n",
    "import warnings\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from pydantic import BaseModel, Field\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    ")\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.schema import Document\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain.callbacks import get_openai_callback\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=1\n",
    "retrieve_k = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2rEYrdZc4iuA"
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "MAXVJsdL4iuA"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(f'../seed{seed}/seed{seed}_train.csv')\n",
    "validation = pd.read_csv(f'../seed{seed}/seed{seed}_validation.csv')\n",
    "test = pd.read_csv(f'../seed{seed}/seed{seed}_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j2bqeHqu4iuA"
   },
   "source": [
    "## Vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "\n",
    "class KUREEmbedding:\n",
    "    def __init__(self, model_name=\"nlpai-lab/KURE-v1\"):\n",
    "        self.model = SentenceTransformer(model_name, trust_remote_code=True).to(device)\n",
    "\n",
    "    def embed_documents(self, texts):\n",
    "        embeddings = self.model.encode(texts, convert_to_numpy=True)\n",
    "        return embeddings\n",
    "\n",
    "    def embed_query(self, text):\n",
    "        return self.embed_documents([text])[0]\n",
    "\n",
    "class KoE5Embedding(KUREEmbedding):\n",
    "    def __init__(self, model_name=\"nlpai-lab/KoE5\"):\n",
    "        super().__init__(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore_path = f\"../seed{seed}/faiss_index_seed{seed}_koe5\"\n",
    "if os.path.exists(vectorstore_path):\n",
    "    embeddings = KoE5Embedding()\n",
    "\n",
    "    vectorstore = FAISS.load_local(\n",
    "        vectorstore_path,\n",
    "        embeddings.embed_query,\n",
    "        allow_dangerous_deserialization=True,\n",
    "    )\n",
    "\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": retrieve_k})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dAzYTg0X4iuC"
   },
   "source": [
    "## Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceClassification(BaseModel):\n",
    "    prediction: str = Field(description=\"Classification of the sentence: True, False\")\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "structured_llm_grader = llm.with_structured_output(SentenceClassification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt & Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiquery_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"Generate two alternative versions of the given sentence in Korean, each within 50 characters.\"),\n",
    "        (\"human\", \"Original sentence: {question}\")\n",
    "    ])\n",
    " \n",
    "retriever_from_llm = MultiQueryRetriever.from_llm(\n",
    "    retriever=vectorstore.as_retriever(),\n",
    "    llm=llm,\n",
    "    prompt=multiquery_prompt\n",
    ")\n",
    "\n",
    "classification_prompt = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", \"Classify the sentence into one of : True , False. \"\n",
    "                \"Judge as True if it is unethical.\\n\"\n",
    "                \"Return in JSON: {{\\\"prediction\\\": \\\"True\\\" or \\\"False\\\"}}\"),\n",
    "     (\"human\", \"Reference sentence: \\n\\n {sentences} \\n\\n Input sentence: {question}\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nGbSqHfkVzu3"
   },
   "source": [
    "## Multi-Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "zuCZsWlvU6z7"
   },
   "outputs": [],
   "source": [
    "def predict_label_with_multiquery_rag(statement: str, idx: int, total: int, max_docs: int = 5):\n",
    "\n",
    "    queries = [statement]\n",
    "    \n",
    "    formatted_prompt = multiquery_prompt.format(question=statement)\n",
    "    generated_queries = llm.invoke(formatted_prompt).content.strip().split(\"\\n\")\n",
    "    queries.extend(generated_queries)\n",
    "    \n",
    "    all_retrieved_docs = []\n",
    "    \n",
    "    for query in queries:\n",
    "        retrieved_docs = retriever_from_llm.invoke(query)[:max_docs]\n",
    "        all_retrieved_docs.extend(retrieved_docs)\n",
    "        \n",
    "    unique_docs = list({doc.page_content: doc for doc in all_retrieved_docs}.values())\n",
    "\n",
    "    retrieved_texts_str = \"\\n\".join([doc.page_content for doc in unique_docs])\n",
    "    \n",
    "    formatted_prompt = classification_prompt.format(question=statement, sentences=retrieved_texts_str)\n",
    "    \n",
    "    prediction = structured_llm_labeler.invoke(formatted_prompt)\n",
    "    \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j9VBQzHTlJr_"
   },
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for idx, row in tqdm(test.iterrows(), total=len(test), desc=\"Processing\"):\n",
    "    question = row[\"문장\"]\n",
    "    answer = row[\"비도덕여부\"]\n",
    "    \n",
    "    prediction = predict_label_with_multiquery_rag(question, idx+1, len(test))\n",
    "\n",
    "    results.append(\n",
    "        {\"question\": question,\n",
    "         \"answer\": answer,\n",
    "         \"prediction\": prediction.prediction,\n",
    "        }\n",
    "    )\n",
    "\n",
    "df_results = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = df_results[\"answer\"].map({True: 1, False: 0})  \n",
    "y_pred = df_results[\"prediction\"].map({\"True\": 1, \"False\": 0})  \n",
    "\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "f1_final = f1_score(y_true, y_pred)\n",
    "f1_macro = f1_score(y_true, y_pred, average=\"macro\")\n",
    "f1_weighted = f1_score(y_true, y_pred, average=\"weighted\")\n",
    "\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "print(\"\\n===== Classification Performance Results =====\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1_final:.4f}\")\n",
    "print(f\"F1-score (Macro): {f1_macro:.4f}\")\n",
    "print(f\"F1-score (Weighted): {f1_weighted:.4f}\")\n",
    "\n",
    "print(\"\\n===== Classification Confusion Matrix =====\")\n",
    "print(conf_matrix)\n",
    "\n",
    "print(\"\\n===== Detailed Classification Report =====\")\n",
    "print(classification_report(y_true, y_pred))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
